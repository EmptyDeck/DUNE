{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from torchvision import transforms\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple device detected\n",
      "Activating Apple Silicon GPU\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import subprocess\n",
    "\n",
    "def GPU():\n",
    "    if torch.cuda.is_available() == True:\n",
    "        device = 'cuda'\n",
    "        templist = [1, 2, 3]\n",
    "        templist = torch.FloatTensor(templist).to(device)\n",
    "        print(\"Cuda torch working : \", end=\"\")\n",
    "        print(templist.is_cuda)\n",
    "        print(\"current device no. : \", end=\"\")\n",
    "        print(torch.cuda.current_device())\n",
    "        print(\"GPU device count : \", end=\"\")\n",
    "        print(torch.cuda.device_count())\n",
    "        print(\"GPU name : \", end=\"\")\n",
    "        print(torch.cuda.get_device_name(0))\n",
    "        print(\"device : \", device)\n",
    "        # Execute the nvidia-smi command using subprocess\n",
    "        try:\n",
    "            output = subprocess.check_output(['nvidia-smi']).decode('utf-8')\n",
    "            print(\"nvidia-smi output:\")\n",
    "            print(output)\n",
    "        except (subprocess.CalledProcessError, FileNotFoundError) as e:\n",
    "            print(\"Error executing nvidia-smi command:\", str(e))\n",
    "    elif torch.backends.mps.is_available() == True:\n",
    "        print(\"Apple device detected\\nActivating Apple Silicon GPU\")\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        print(\"cant use gpu , activating cpu\")\n",
    "        device = 'cpu'\n",
    "\n",
    "    return device\n",
    "device = GPU()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use 3 channel Resnet\n",
    "class MonoToColor(nn.Module):\n",
    "    def __init__(self, num_channels=3):\n",
    "        super(MonoToColor, self).__init__()\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        return tensor.repeat(self.num_channels, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 클래스 정의\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class UrbanSoundDataset(Dataset):\n",
    "    def __init__(self, annotations_file, audio_dir, transformation, target_sample_rate):\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.transformation = transformation\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        \n",
    "        # Directly assign class IDs without filtering on selected labels\n",
    "        self.annotations['classID'] = self.annotations['class'].astype('category').cat.codes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self._get_audio_sample_label(index)\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        signal = self.transformation(signal)\n",
    "        return signal, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.target_sample_rate:\n",
    "            signal = signal[:, :self.target_sample_rate]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.target_sample_rate:\n",
    "            num_missing_samples = self.target_sample_rate - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "\n",
    "    def _get_audio_sample_path(self, index):\n",
    "        fold = self.annotations.iloc[index, 5]\n",
    "        file_name = self.annotations.iloc[index, 0]\n",
    "        #audio_sample_path = os.path.join(self.audio_dir, fold, file_name)\n",
    "        audio_sample_path = os.path.join(\"../../../../../검은 상자/@CODE\",self.audio_dir, fold, file_name)\n",
    "        return audio_sample_path\n",
    "\n",
    "    def _get_audio_sample_label(self, index):\n",
    "        return self.annotations.iloc[index, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>children_cut_0000.wav</td>\n",
       "      <td>540970</td>\n",
       "      <td>259.733605</td>\n",
       "      <td>261.733605</td>\n",
       "      <td>1</td>\n",
       "      <td>children</td>\n",
       "      <td>0</td>\n",
       "      <td>children</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>children_cut_0001.wav</td>\n",
       "      <td>548447</td>\n",
       "      <td>3183.482744</td>\n",
       "      <td>3185.482744</td>\n",
       "      <td>1</td>\n",
       "      <td>children</td>\n",
       "      <td>0</td>\n",
       "      <td>children</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>children_cut_0002.wav</td>\n",
       "      <td>883691</td>\n",
       "      <td>237.423311</td>\n",
       "      <td>239.423311</td>\n",
       "      <td>1</td>\n",
       "      <td>children</td>\n",
       "      <td>0</td>\n",
       "      <td>children</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>children_cut_0003.wav</td>\n",
       "      <td>661176</td>\n",
       "      <td>1907.488526</td>\n",
       "      <td>1909.488526</td>\n",
       "      <td>1</td>\n",
       "      <td>children</td>\n",
       "      <td>0</td>\n",
       "      <td>children</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>children_cut_0004.wav</td>\n",
       "      <td>969140</td>\n",
       "      <td>3735.553537</td>\n",
       "      <td>3737.553537</td>\n",
       "      <td>1</td>\n",
       "      <td>children</td>\n",
       "      <td>0</td>\n",
       "      <td>children</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>children_cut_0005.wav</td>\n",
       "      <td>742221</td>\n",
       "      <td>684.140272</td>\n",
       "      <td>686.140272</td>\n",
       "      <td>1</td>\n",
       "      <td>children</td>\n",
       "      <td>0</td>\n",
       "      <td>children</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>children_cut_0006.wav</td>\n",
       "      <td>776059</td>\n",
       "      <td>2565.087755</td>\n",
       "      <td>2567.087755</td>\n",
       "      <td>1</td>\n",
       "      <td>children</td>\n",
       "      <td>0</td>\n",
       "      <td>children</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>children_cut_0007.wav</td>\n",
       "      <td>265473</td>\n",
       "      <td>3600.593900</td>\n",
       "      <td>3602.593900</td>\n",
       "      <td>1</td>\n",
       "      <td>children</td>\n",
       "      <td>0</td>\n",
       "      <td>children</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>children_cut_0008.wav</td>\n",
       "      <td>582832</td>\n",
       "      <td>1581.066780</td>\n",
       "      <td>1583.066780</td>\n",
       "      <td>1</td>\n",
       "      <td>children</td>\n",
       "      <td>0</td>\n",
       "      <td>children</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>children_cut_0009.wav</td>\n",
       "      <td>891737</td>\n",
       "      <td>1144.900794</td>\n",
       "      <td>1146.900794</td>\n",
       "      <td>1</td>\n",
       "      <td>children</td>\n",
       "      <td>0</td>\n",
       "      <td>children</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         slice_file_name    fsID        start          end  salience  \\\n",
       "0  children_cut_0000.wav  540970   259.733605   261.733605         1   \n",
       "1  children_cut_0001.wav  548447  3183.482744  3185.482744         1   \n",
       "2  children_cut_0002.wav  883691   237.423311   239.423311         1   \n",
       "3  children_cut_0003.wav  661176  1907.488526  1909.488526         1   \n",
       "4  children_cut_0004.wav  969140  3735.553537  3737.553537         1   \n",
       "5  children_cut_0005.wav  742221   684.140272   686.140272         1   \n",
       "6  children_cut_0006.wav  776059  2565.087755  2567.087755         1   \n",
       "7  children_cut_0007.wav  265473  3600.593900  3602.593900         1   \n",
       "8  children_cut_0008.wav  582832  1581.066780  1583.066780         1   \n",
       "9  children_cut_0009.wav  891737  1144.900794  1146.900794         1   \n",
       "\n",
       "       fold  classID     class  \n",
       "0  children        0  children  \n",
       "1  children        0  children  \n",
       "2  children        0  children  \n",
       "3  children        0  children  \n",
       "4  children        0  children  \n",
       "5  children        0  children  \n",
       "6  children        0  children  \n",
       "7  children        0  children  \n",
       "8  children        0  children  \n",
       "9  children        0  children  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋 및 데이터 로더 설정\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "ANNOTATIONS_FILE = '/Volumes/검은 상자/@CODE/dataset2/data2.csv'\n",
    "temp_csv = pd.read_csv('/Volumes/검은 상자/@CODE/dataset2/data2.csv')\n",
    "temp_csv.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "children        1000\n",
      "nothing2        1000\n",
      "drilling        1000\n",
      "engine          1000\n",
      "siren           1000\n",
      "gunshot         1000\n",
      "aircon          1000\n",
      "jackhammer      1000\n",
      "carhorn         1000\n",
      "glass           1000\n",
      "nock            1000\n",
      "street_music    1000\n",
      "dog_bark        1000\n",
      "nothing1        1000\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have the ANNOTATIONS_FILE dataframe\n",
    "# You can use the value_counts() function on the 'class' column\n",
    "label_counts = temp_csv['class'].value_counts()\n",
    "\n",
    "# Display the counts of files for each label in a tabular format\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "random = 1 # 1== random,  0 == fix seed\n",
    "\n",
    "if random == 0:\n",
    "    import random\n",
    "    import numpy as np\n",
    "\n",
    "    seed = 42\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_DIR = 'dataset2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 22050\n",
    "BATCH_SIZE = 1\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxNormalize(nn.Module):\n",
    "    def __init__(self, min_val=None, max_val=None):\n",
    "        super(MinMaxNormalize, self).__init__()\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        if self.min_val is None or self.max_val is None:\n",
    "            min_val = torch.min(tensor)\n",
    "            max_val = torch.max(tensor)\n",
    "        else:\n",
    "            min_val = self.min_val\n",
    "            max_val = self.max_val\n",
    "        \n",
    "        normalized_tensor = (tensor - min_val) / (max_val - min_val)\n",
    "        return normalized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation = transforms.Compose([\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_mels=40),# higher the better but more complex. For talking we use 128, for sound effect, about 40.\n",
    "    torchaudio.transforms.AmplitudeToDB(stype='power', top_db=80),\n",
    "    MinMaxNormalize(),\n",
    "    MonoToColor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "usd = UrbanSoundDataset(ANNOTATIONS_FILE, AUDIO_DIR, transformation, SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 분리\n",
    "dataset_size = len(usd)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = int(0.1 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(usd, [train_size, val_size, test_size])\n",
    "\n",
    "# 데이터 로더 생성\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=True,\n",
    "                            num_workers=NUM_WORKERS,\n",
    "                            pin_memory=PIN_MEMORY)\n",
    "\n",
    "val_loader = DataLoader(dataset=val_dataset,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=True,\n",
    "                            num_workers=NUM_WORKERS,\n",
    "                            pin_memory=PIN_MEMORY)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=True,\n",
    "                            num_workers=NUM_WORKERS,\n",
    "                            pin_memory=PIN_MEMORY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/검은 상자/@CODE/dataset2/data2.csv'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANNOTATIONS_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.UrbanSoundDataset at 0x2b379ea10>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = len(usd.annotations['classID'].unique())\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owo/anaconda3/envs/torchenv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/owo/anaconda3/envs/torchenv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# ResNet18 모델 설정\n",
    "model = resnet18(pretrained=False)\n",
    "# Change the output neurons of the model\n",
    "model.fc = nn.Linear(512,num_labels) \n",
    "# Use multiple GPUs if available\n",
    "if torch.cuda.device_count() > 1:\n",
    "  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "  model = nn.DataParallel(model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수와 옵티마이저 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습률 스케줄러 설정\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train func 2 (more normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.train_model2(model, dataloaders, criterion, optimizer, scheduler, num_epochs)>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model2(model, dataloaders, criterion, optimizer, scheduler, num_epochs):\n",
    "    global best_acc\n",
    "    global train_loss_list, val_loss_list, train_acc_list, val_acc_list\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    train_acc_list = []\n",
    "    val_acc_list = []\n",
    "    best_acc = 0.0\n",
    "    best_model_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Training phase\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss_train = 0.0\n",
    "        running_corrects_train = 0\n",
    "\n",
    "        for inputs, labels in tqdm(dataloaders['train']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate statistics\n",
    "            running_loss_train += loss.item() * inputs.size(0)\n",
    "            running_corrects_train += torch.sum(preds == labels.data)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss_train = running_loss_train / dataset_sizes['train']\n",
    "        epoch_acc_train = running_corrects_train.float() / dataset_sizes['train']\n",
    "\n",
    "        print('Train Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss_train, epoch_acc_train))\n",
    "\n",
    "        # Append to the train lists\n",
    "        train_loss_list.append(epoch_loss_train)\n",
    "        train_acc_list.append(epoch_acc_train.item())\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        running_loss_val = 0.0\n",
    "        running_corrects_val = 0\n",
    "\n",
    "        for inputs, labels in tqdm(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            # Calculate statistics\n",
    "            running_loss_val += loss.item() * inputs.size(0)\n",
    "            running_corrects_val += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss_val = running_loss_val / dataset_sizes['val']\n",
    "        epoch_acc_val = running_corrects_val.float() / dataset_sizes['val']\n",
    "\n",
    "        print('Validation Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss_val, epoch_acc_val))\n",
    "\n",
    "        # Append to the validation lists\n",
    "        val_loss_list.append(epoch_loss_val)\n",
    "        val_acc_list.append(epoch_acc_val.item())\n",
    "\n",
    "        if epoch_acc_val > best_acc:\n",
    "            best_acc = epoch_acc_val\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            best_model_counter += 1\n",
    "            formatted_counter = str(best_model_counter).zfill(3)\n",
    "            model_save_path = os.path.join(\"models\", f\"Model2_{formatted_counter}.pth\")\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f'Best model saved at {model_save_path}')\n",
    "\n",
    "    print('Best Validation Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# This is the updated function\n",
    "train_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:40<00:00, 13.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2502 Acc: 0.5668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.9901 Acc: 0.1607\n",
      "Best model saved at models/Model2_001.pth\n",
      "Epoch 2/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [14:07<00:00, 13.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6591 Acc: 0.7684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4.1404 Acc: 0.2579\n",
      "Best model saved at models/Model2_002.pth\n",
      "Epoch 3/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [14:00<00:00, 13.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4432 Acc: 0.8495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.0394 Acc: 0.1621\n",
      "Epoch 4/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:55<00:00, 13.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3191 Acc: 0.8879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4.9808 Acc: 0.1921\n",
      "Epoch 5/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:43<00:00, 13.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2313 Acc: 0.9205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:48<00:00, 28.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 9.1016 Acc: 0.1771\n",
      "Epoch 6/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:44<00:00, 13.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1595 Acc: 0.9451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 7.8587 Acc: 0.1857\n",
      "Epoch 7/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:46<00:00, 13.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1144 Acc: 0.9608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 9.2662 Acc: 0.2693\n",
      "Best model saved at models/Model2_003.pth\n",
      "Epoch 8/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [14:00<00:00, 13.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0257 Acc: 0.9936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 8.4813 Acc: 0.2914\n",
      "Best model saved at models/Model2_004.pth\n",
      "Epoch 9/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:37<00:00, 13.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0048 Acc: 0.9997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 10.1219 Acc: 0.3164\n",
      "Best model saved at models/Model2_005.pth\n",
      "Epoch 10/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:44<00:00, 13.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0011 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 9.4004 Acc: 0.2929\n",
      "Epoch 11/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:45<00:00, 13.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0003 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 11.5614 Acc: 0.3229\n",
      "Best model saved at models/Model2_006.pth\n",
      "Epoch 12/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:27<00:00, 13.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0001 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 10.3397 Acc: 0.3300\n",
      "Best model saved at models/Model2_007.pth\n",
      "Epoch 13/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:08<00:00, 14.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 11.2590 Acc: 0.2914\n",
      "Epoch 14/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:05<00:00, 14.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 10.1373 Acc: 0.3843\n",
      "Best model saved at models/Model2_008.pth\n",
      "Epoch 15/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:06<00:00, 14.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 11.8182 Acc: 0.3036\n",
      "Epoch 16/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:05<00:00, 14.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 12.3275 Acc: 0.3443\n",
      "Epoch 17/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:07<00:00, 14.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 11.2495 Acc: 0.3421\n",
      "Epoch 18/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:06<00:00, 14.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 10.0885 Acc: 0.3679\n",
      "Epoch 19/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:05<00:00, 14.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 9.6334 Acc: 0.3379\n",
      "Epoch 20/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:07<00:00, 14.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 10.7611 Acc: 0.3421\n",
      "Epoch 21/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:08<00:00, 14.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 11.7097 Acc: 0.3493\n",
      "Epoch 22/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:07<00:00, 14.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 11.9538 Acc: 0.3286\n",
      "Epoch 23/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:06<00:00, 14.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 13.7118 Acc: 0.2971\n",
      "Epoch 24/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:06<00:00, 14.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 13.2624 Acc: 0.2464\n",
      "Epoch 25/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:07<00:00, 14.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 11.6208 Acc: 0.3736\n",
      "Epoch 26/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:06<00:00, 14.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 10.2815 Acc: 0.3900\n",
      "Best model saved at models/Model2_009.pth\n",
      "Epoch 27/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:08<00:00, 14.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 10.3822 Acc: 0.3993\n",
      "Best model saved at models/Model2_010.pth\n",
      "Epoch 28/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:08<00:00, 14.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 10.5380 Acc: 0.3643\n",
      "Epoch 29/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:08<00:00, 14.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 11.8579 Acc: 0.3993\n",
      "Epoch 30/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:07<00:00, 14.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 10.4156 Acc: 0.3621\n",
      "Epoch 31/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:07<00:00, 14.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 11.1138 Acc: 0.3807\n",
      "Epoch 32/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:07<00:00, 14.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 10.7444 Acc: 0.3586\n",
      "Epoch 33/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:07<00:00, 14.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 11.4409 Acc: 0.3614\n",
      "Epoch 34/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:08<00:00, 14.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 10.0517 Acc: 0.3700\n",
      "Epoch 35/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:07<00:00, 14.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 12.6612 Acc: 0.3036\n",
      "Epoch 36/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:08<00:00, 14.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 11.6283 Acc: 0.3721\n",
      "Epoch 37/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:07<00:00, 14.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 12.5423 Acc: 0.3314\n",
      "Epoch 38/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:09<00:00, 14.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 10.8835 Acc: 0.3607\n",
      "Epoch 39/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:08<00:00, 14.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 11.6193 Acc: 0.3093\n",
      "Epoch 40/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:06<00:00, 14.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 10.4044 Acc: 0.3300\n",
      "Epoch 41/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:09<00:00, 14.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 11.3801 Acc: 0.3357\n",
      "Epoch 42/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11200/11200 [13:11<00:00, 14.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:47<00:00, 29.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 12.7452 Acc: 0.2936\n",
      "Epoch 43/1000\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2509/11200 [03:01<10:28, 13.83it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[142], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m# define dataset_sizes\u001b[39;00m\n\u001b[1;32m      6\u001b[0m dataset_sizes \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mlen\u001b[39m(train_dataset), \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mlen\u001b[39m(val_dataset)}\n\u001b[0;32m----> 8\u001b[0m best_model \u001b[39m=\u001b[39m train_model2(model, dataloaders, criterion, optimizer, scheduler, num_epochs\u001b[39m=\u001b[39mnb_epochs)\n",
      "Cell \u001b[0;32mIn[141], line 30\u001b[0m, in \u001b[0;36mtrain_model2\u001b[0;34m(model, dataloaders, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     27\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     29\u001b[0m \u001b[39m# Forward\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     31\u001b[0m _, preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs, \u001b[39m1\u001b[39m)\n\u001b[1;32m     32\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_impl(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/torchvision/models/resnet.py:275\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    273\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(x)\n\u001b[1;32m    274\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(x)\n\u001b[0;32m--> 275\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(x)\n\u001b[1;32m    276\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer4(x)\n\u001b[1;32m    278\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/torchvision/models/resnet.py:92\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m     90\u001b[0m     identity \u001b[39m=\u001b[39m x\n\u001b[0;32m---> 92\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)\n\u001b[1;32m     93\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(out)\n\u001b[1;32m     94\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conv_forward(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(\u001b[39minput\u001b[39m, weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import copy\n",
    "nb_epochs = 1000\n",
    "# 모델 훈련 시작\n",
    "dataloaders = {\"train\": train_loader, \"val\": val_loader}\n",
    "# define dataset_sizes\n",
    "dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n",
    "\n",
    "best_model = train_model2(model, dataloaders, criterion, optimizer, scheduler, num_epochs=nb_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_list = torch.tensor(train_acc_list).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc_list = torch.tensor(val_acc_list).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (1000,) and (42,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[147], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m12\u001b[39m, \u001b[39m6\u001b[39m))\n\u001b[1;32m      5\u001b[0m plt\u001b[39m.\u001b[39msubplot(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m plt\u001b[39m.\u001b[39mplot(\u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, nb_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), train_loss_list, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTrain\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m plt\u001b[39m.\u001b[39mplot(\u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, nb_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), val_loss_list, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mValidation\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m'\u001b[39m\u001b[39mEpoch\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/matplotlib/pyplot.py:2812\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2810\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mplot)\n\u001b[1;32m   2811\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot\u001b[39m(\u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, scaley\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 2812\u001b[0m     \u001b[39mreturn\u001b[39;00m gca()\u001b[39m.\u001b[39mplot(\n\u001b[1;32m   2813\u001b[0m         \u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39mscalex, scaley\u001b[39m=\u001b[39mscaley,\n\u001b[1;32m   2814\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m({\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m: data} \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/matplotlib/axes/_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1685\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[0;32m-> 1688\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[1;32m   1689\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[1;32m   1690\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/matplotlib/axes/_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[1;32m    310\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 311\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_plot_args(\n\u001b[1;32m    312\u001b[0m     this, kwargs, ambiguous_fmt_datakey\u001b[39m=\u001b[39mambiguous_fmt_datakey)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/matplotlib/axes/_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39myaxis\u001b[39m.\u001b[39mupdate_units(y)\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[0;32m--> 504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y must have same first dimension, but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhave shapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    506\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    507\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y can be no greater than 2D, but have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1000,) and (42,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training and validation loss and accuracy\n",
    "# Move tensors to CPU\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, nb_epochs + 1), train_loss_list, label='Train')\n",
    "plt.plot(range(1, nb_epochs + 1), val_loss_list, label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, nb_epochs + 1), train_acc_list, label='Train')\n",
    "plt.plot(range(1, nb_epochs + 1), val_acc_list, label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가 함수 정의\n",
    "def test_model(model, test_loader, device):\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # 그래디언트 계산 비활성화\n",
    "        for inputs, labels in tqdm(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the model on test images: {100 * correct / total}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:49<00:00, 28.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on test images: 26.071428571428573%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 훈련 및 평가\n",
    "test_model(best_model, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
