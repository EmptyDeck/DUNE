{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1019,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from torchvision import transforms\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple device detected\n",
      "Activating Apple Silicon GPU\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "##IDK what is wrong with that up cell\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "def GPU():\n",
    "    if torch.cuda.is_available() == True:\n",
    "        device = 'cuda'\n",
    "        templist = [1, 2, 3]\n",
    "        templist = torch.FloatTensor(templist).to(device)\n",
    "        print(\"Cuda torch working : \", end=\"\")\n",
    "        print(templist.is_cuda)\n",
    "        print(\"current device no. : \", end=\"\")\n",
    "        print(torch.cuda.current_device())\n",
    "        print(\"GPU device count : \", end=\"\")\n",
    "        print(torch.cuda.device_count())\n",
    "        print(\"GPU name : \", end=\"\")\n",
    "        print(torch.cuda.get_device_name(0))\n",
    "        print(\"device : \", device)\n",
    "        # Execute the nvidia-smi command using subprocess\n",
    "        try:\n",
    "            output = subprocess.check_output(['nvidia-smi']).decode('utf-8')\n",
    "            print(\"nvidia-smi output:\")\n",
    "            print(output)\n",
    "        except (subprocess.CalledProcessError, FileNotFoundError) as e:\n",
    "            print(\"Error executing nvidia-smi command:\", str(e))\n",
    "    elif torch.backends.mps.is_available() == True:\n",
    "        print(\"Apple device detected\\nActivating Apple Silicon GPU\")\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        print(\"cant use gpu , activating cpu\")\n",
    "        device = 'cpu'\n",
    "\n",
    "    return device\n",
    "device = GPU()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use 3 channel Resnet\n",
    "class MonoToColor(nn.Module):\n",
    "    def __init__(self, num_channels=3):\n",
    "        super(MonoToColor, self).__init__()\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        return tensor.repeat(self.num_channels, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 클래스 정의\n",
    "class UrbanSoundDataset(Dataset):\n",
    "    def __init__(self, annotations_file, audio_dir, transformation, target_sample_rate, selected_labels):\n",
    "        self.annotations = annotations_file\n",
    "        self.audio_dir = audio_dir\n",
    "        self.transformation = transformation\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.selected_labels = selected_labels\n",
    "\n",
    "        self.annotations = self.annotations[self.annotations['class'].isin(self.selected_labels)]\n",
    "        self.annotations['classID'] = self.annotations['class'].astype('category').cat.codes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self._get_audio_sample_label(index)\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        signal = self.transformation(signal)\n",
    "        return signal, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.target_sample_rate:\n",
    "            signal = signal[:, :self.target_sample_rate]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.target_sample_rate:\n",
    "            num_missing_samples = self.target_sample_rate - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "    def _get_audio_sample_path(self, index):\n",
    "        fold = f\"fold{self.annotations.iloc[index, 5]}\"\n",
    "        file_name = self.annotations.iloc[index, 0]\n",
    "        audio_sample_path = os.path.join(self.audio_dir, fold, file_name)\n",
    "        return audio_sample_path\n",
    "    def _get_audio_sample_label(self, index):\n",
    "        return self.annotations.iloc[index, 6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>glass_shatter_0.wav</td>\n",
       "      <td>111111</td>\n",
       "      <td>3339.908526</td>\n",
       "      <td>3341.908526</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>glass_shatter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>glass_shatter_1.wav</td>\n",
       "      <td>111111</td>\n",
       "      <td>42.272744</td>\n",
       "      <td>44.272744</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>glass_shatter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>glass_shatter_2.wav</td>\n",
       "      <td>111111</td>\n",
       "      <td>2771.678095</td>\n",
       "      <td>2773.678095</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>glass_shatter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>glass_shatter_3.wav</td>\n",
       "      <td>111111</td>\n",
       "      <td>232.600317</td>\n",
       "      <td>234.600317</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>glass_shatter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>glass_shatter_4.wav</td>\n",
       "      <td>111111</td>\n",
       "      <td>1917.612925</td>\n",
       "      <td>1919.612925</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>glass_shatter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       slice_file_name    fsID        start          end  salience  fold  \\\n",
       "0  glass_shatter_0.wav  111111  3339.908526  3341.908526         1    11   \n",
       "1  glass_shatter_1.wav  111111    42.272744    44.272744         1    11   \n",
       "2  glass_shatter_2.wav  111111  2771.678095  2773.678095         1    11   \n",
       "3  glass_shatter_3.wav  111111   232.600317   234.600317         1    11   \n",
       "4  glass_shatter_4.wav  111111  1917.612925  1919.612925         1    11   \n",
       "\n",
       "   classID          class  \n",
       "0       11  glass_shatter  \n",
       "1       11  glass_shatter  \n",
       "2       11  glass_shatter  \n",
       "3       11  glass_shatter  \n",
       "4       11  glass_shatter  "
      ]
     },
     "execution_count": 1023,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## New label\n",
    "\n",
    "# Load the CSV data for the 'glass_shatter' class\n",
    "data_glass_shatter = pd.read_csv(\"/Users/owo/HOUSE/@DUNE/@AI/Sound Classification/newsound/UrbanSound8K_for_glassshatter.csv\")\n",
    "data_glass_shatter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1024,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>door_nock_0.wav</td>\n",
       "      <td>222222</td>\n",
       "      <td>2998.994467</td>\n",
       "      <td>3000.994467</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>door_nock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>door_nock_1.wav</td>\n",
       "      <td>222222</td>\n",
       "      <td>665.285034</td>\n",
       "      <td>667.285034</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>door_nock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>door_nock_2.wav</td>\n",
       "      <td>222222</td>\n",
       "      <td>478.739955</td>\n",
       "      <td>480.739955</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>door_nock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>door_nock_3.wav</td>\n",
       "      <td>222222</td>\n",
       "      <td>259.646757</td>\n",
       "      <td>261.646757</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>door_nock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>door_nock_4.wav</td>\n",
       "      <td>222222</td>\n",
       "      <td>965.875782</td>\n",
       "      <td>967.875782</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>door_nock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   slice_file_name    fsID        start          end  salience  fold  classID  \\\n",
       "0  door_nock_0.wav  222222  2998.994467  3000.994467         1    11       12   \n",
       "1  door_nock_1.wav  222222   665.285034   667.285034         1    11       12   \n",
       "2  door_nock_2.wav  222222   478.739955   480.739955         1    11       12   \n",
       "3  door_nock_3.wav  222222   259.646757   261.646757         1    11       12   \n",
       "4  door_nock_4.wav  222222   965.875782   967.875782         1    11       12   \n",
       "\n",
       "       class  \n",
       "0  door_nock  \n",
       "1  door_nock  \n",
       "2  door_nock  \n",
       "3  door_nock  \n",
       "4  door_nock  "
      ]
     },
     "execution_count": 1024,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV data for the 'door_nock' class\n",
    "data_door_nock = pd.read_csv(\"/Users/owo/HOUSE/@DUNE/@AI/Sound Classification/newsound/UrbanSound8K_for_doornock.csv\")\n",
    "data_door_nock.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1025,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nothing_0.wav</td>\n",
       "      <td>333333</td>\n",
       "      <td>78.249615</td>\n",
       "      <td>80.249615</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nothing_1.wav</td>\n",
       "      <td>333333</td>\n",
       "      <td>641.007846</td>\n",
       "      <td>643.007846</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nothing_2.wav</td>\n",
       "      <td>333333</td>\n",
       "      <td>3068.871111</td>\n",
       "      <td>3070.871111</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nothing_3.wav</td>\n",
       "      <td>333333</td>\n",
       "      <td>1948.704762</td>\n",
       "      <td>1950.704762</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nothing_4.wav</td>\n",
       "      <td>333333</td>\n",
       "      <td>2462.373787</td>\n",
       "      <td>2464.373787</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>nothing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  slice_file_name    fsID        start          end  salience  fold  classID  \\\n",
       "0   nothing_0.wav  333333    78.249615    80.249615         1    11       13   \n",
       "1   nothing_1.wav  333333   641.007846   643.007846         1    11       13   \n",
       "2   nothing_2.wav  333333  3068.871111  3070.871111         1    11       13   \n",
       "3   nothing_3.wav  333333  1948.704762  1950.704762         1    11       13   \n",
       "4   nothing_4.wav  333333  2462.373787  2464.373787         1    11       13   \n",
       "\n",
       "     class  \n",
       "0  nothing  \n",
       "1  nothing  \n",
       "2  nothing  \n",
       "3  nothing  \n",
       "4  nothing  "
      ]
     },
     "execution_count": 1025,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV data for the 'nothing' class\n",
    "data_nothing = pd.read_csv(\"/Users/owo/HOUSE/@DUNE/@AI/Sound Classification/newsound/UrbanSound8K_for_nothing.csv\")\n",
    "data_nothing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1026,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nothing2_0.wav</td>\n",
       "      <td>444444</td>\n",
       "      <td>2.109116</td>\n",
       "      <td>4.109116</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>nothing2</td>\n",
       "      <td>nothing2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nothing2_1.wav</td>\n",
       "      <td>444444</td>\n",
       "      <td>5.310340</td>\n",
       "      <td>7.310340</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>nothing2</td>\n",
       "      <td>nothing2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nothing2_2.wav</td>\n",
       "      <td>444444</td>\n",
       "      <td>14.283356</td>\n",
       "      <td>16.283356</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>nothing2</td>\n",
       "      <td>nothing2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nothing2_3.wav</td>\n",
       "      <td>444444</td>\n",
       "      <td>18.015828</td>\n",
       "      <td>20.015828</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>nothing2</td>\n",
       "      <td>nothing2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nothing2_4.wav</td>\n",
       "      <td>444444</td>\n",
       "      <td>3.787800</td>\n",
       "      <td>5.787800</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>nothing2</td>\n",
       "      <td>nothing2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  slice_file_name    fsID      start        end  salience  fold   classID  \\\n",
       "0  nothing2_0.wav  444444   2.109116   4.109116         1    11  nothing2   \n",
       "1  nothing2_1.wav  444444   5.310340   7.310340         1    11  nothing2   \n",
       "2  nothing2_2.wav  444444  14.283356  16.283356         1    11  nothing2   \n",
       "3  nothing2_3.wav  444444  18.015828  20.015828         1    11  nothing2   \n",
       "4  nothing2_4.wav  444444   3.787800   5.787800         1    11  nothing2   \n",
       "\n",
       "      class  \n",
       "0  nothing2  \n",
       "1  nothing2  \n",
       "2  nothing2  \n",
       "3  nothing2  \n",
       "4  nothing2  "
      ]
     },
     "execution_count": 1026,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV data for the 'nothing' class\n",
    "data_nothing2 = pd.read_csv(\"/Users/owo/HOUSE/@DUNE/@AI/Sound Classification/newsound/UrbanSound8K_for_nothing2.csv\")\n",
    "data_nothing2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1027,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 및 데이터 로더 설정\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    ANNOTATIONS_FILE = pd.read_csv('/Users/owo/HOUSE/@DUNE/@AI/Sound Classification/archive/UrbanSound8K.csv')\n",
    "except:\n",
    "    try:\n",
    "        ANNOTATIONS_FILE = pd.read_csv('/Users/owo/sound_datasets/urbansound8k/UrbanSound8K/metadata/UrbanSound8K.csv')\n",
    "    except:\n",
    "        ANNOTATIONS_FILE = pd.read_csv('C:/Users/PC/AppData/@FOLDER/@Project/UrbanSound8K/metadata/UrbanSound8K.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1028,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100032-3-0-0.wav</td>\n",
       "      <td>100032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.317551</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100263-2-0-117.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>58.500000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100263-2-0-121.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>60.500000</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100263-2-0-126.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100263-2-0-137.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>68.500000</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11727</th>\n",
       "      <td>nothing_995.wav</td>\n",
       "      <td>333333</td>\n",
       "      <td>596.725488</td>\n",
       "      <td>598.725488</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11728</th>\n",
       "      <td>nothing_996.wav</td>\n",
       "      <td>333333</td>\n",
       "      <td>1736.118639</td>\n",
       "      <td>1738.118639</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11729</th>\n",
       "      <td>nothing_997.wav</td>\n",
       "      <td>333333</td>\n",
       "      <td>1472.959002</td>\n",
       "      <td>1474.959002</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11730</th>\n",
       "      <td>nothing_998.wav</td>\n",
       "      <td>333333</td>\n",
       "      <td>248.747211</td>\n",
       "      <td>250.747211</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11731</th>\n",
       "      <td>nothing_999.wav</td>\n",
       "      <td>333333</td>\n",
       "      <td>1475.270703</td>\n",
       "      <td>1477.270703</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>nothing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11732 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          slice_file_name    fsID        start          end  salience  fold  \\\n",
       "0        100032-3-0-0.wav  100032     0.000000     0.317551         1     5   \n",
       "1      100263-2-0-117.wav  100263    58.500000    62.500000         1     5   \n",
       "2      100263-2-0-121.wav  100263    60.500000    64.500000         1     5   \n",
       "3      100263-2-0-126.wav  100263    63.000000    67.000000         1     5   \n",
       "4      100263-2-0-137.wav  100263    68.500000    72.500000         1     5   \n",
       "...                   ...     ...          ...          ...       ...   ...   \n",
       "11727     nothing_995.wav  333333   596.725488   598.725488         1    11   \n",
       "11728     nothing_996.wav  333333  1736.118639  1738.118639         1    11   \n",
       "11729     nothing_997.wav  333333  1472.959002  1474.959002         1    11   \n",
       "11730     nothing_998.wav  333333   248.747211   250.747211         1    11   \n",
       "11731     nothing_999.wav  333333  1475.270703  1477.270703         1    11   \n",
       "\n",
       "       classID             class  \n",
       "0            3          dog_bark  \n",
       "1            2  children_playing  \n",
       "2            2  children_playing  \n",
       "3            2  children_playing  \n",
       "4            2  children_playing  \n",
       "...        ...               ...  \n",
       "11727       13           nothing  \n",
       "11728       13           nothing  \n",
       "11729       13           nothing  \n",
       "11730       13           nothing  \n",
       "11731       13           nothing  \n",
       "\n",
       "[11732 rows x 8 columns]"
      ]
     },
     "execution_count": 1028,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANNOTATIONS_FILE = pd.concat([ANNOTATIONS_FILE, data_glass_shatter, data_door_nock,data_nothing])\n",
    "ANNOTATIONS_FILE = ANNOTATIONS_FILE.reset_index(drop=True)\n",
    "ANNOTATIONS_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1029,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog_bark            1000\n",
      "children_playing    1000\n",
      "air_conditioner     1000\n",
      "street_music        1000\n",
      "engine_idling       1000\n",
      "jackhammer          1000\n",
      "drilling            1000\n",
      "glass_shatter       1000\n",
      "door_nock           1000\n",
      "nothing             1000\n",
      "siren                929\n",
      "car_horn             429\n",
      "gun_shot             374\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have the ANNOTATIONS_FILE dataframe\n",
    "# You can use the value_counts() function on the 'class' column\n",
    "label_counts = ANNOTATIONS_FILE['class'].value_counts()\n",
    "\n",
    "# Display the counts of files for each label in a tabular format\n",
    "print(label_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1030,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10e0c3970>"
      ]
     },
     "execution_count": 1030,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1031,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog_bark            1000\n",
      "children_playing    1000\n",
      "air_conditioner     1000\n",
      "street_music        1000\n",
      "engine_idling       1000\n",
      "jackhammer          1000\n",
      "drilling            1000\n",
      "glass_shatter       1000\n",
      "door_nock           1000\n",
      "nothing             1000\n",
      "siren                929\n",
      "car_horn             429\n",
      "gun_shot             374\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have the ANNOTATIONS_FILE dataframe\n",
    "# You can use the value_counts() function on the 'class' column\n",
    "label_counts = ANNOTATIONS_FILE['class'].value_counts()\n",
    "\n",
    "# Display the counts of files for each label in a tabular format\n",
    "print(label_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1032,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog_bark            1000\n",
      "children_playing    1000\n",
      "air_conditioner     1000\n",
      "street_music        1000\n",
      "engine_idling       1000\n",
      "jackhammer          1000\n",
      "drilling            1000\n",
      "glass_shatter       1000\n",
      "door_nock           1000\n",
      "nothing             1000\n",
      "siren                929\n",
      "car_horn             429\n",
      "gun_shot             374\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have the ANNOTATIONS_FILE dataframe\n",
    "# You can use the value_counts() function on the 'class' column\n",
    "label_counts = ANNOTATIONS_FILE['class'].value_counts()\n",
    "\n",
    "# Display the counts of files for each label in a tabular format\n",
    "print(label_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1033,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUDIO_DIR = \"/Users/cafalena/sound_datasets/urbansound8k/UrbanSound8K/audio\"##cafalena or owo\n",
    "#AUDIO_DIR = \"C:/Users/PC/AppData/@FOLDER/@Project/UrbanSound8K/audio\"##cafalena or owo\n",
    "AUDIO_DIR = \"/Users/owo/HOUSE/@DUNE/@AI/Sound Classification/archive\"##cafalena or owo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1034,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 22050\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owo/anaconda3/envs/torchenv/lib/python3.11/site-packages/torchaudio/functional/functional.py:576: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "transformation = transforms.Compose([\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_mels=128),\n",
    "    torchaudio.transforms.AmplitudeToDB(stype='power', top_db=80),\n",
    "    MonoToColor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1036,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected_labels = [\"air_conditioner\", \"car_horn\", \"children_playing\", \"dog_bark\",\"drilling\", \"engine_idling\", \"gun_shot\", \"jackhammer\", \"siren\", \"street_music\", \"glass_shatter\", \"door_nock\",\"nothing\"]\n",
    "\n",
    "selected_labels = [\"door_nock\",\"glass_shatter\",\"car_horn\",\"dog_bark\",\"drilling\",\"nothing\",\"siren\",\"nothing2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1037,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sb/2gzn9_qx589_ft0jp8q6nbbc0000gn/T/ipykernel_89956/231203154.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.annotations['classID'] = self.annotations['class'].astype('category').cat.codes\n"
     ]
    }
   ],
   "source": [
    "usd = UrbanSoundDataset(ANNOTATIONS_FILE, AUDIO_DIR, transformation, SAMPLE_RATE, selected_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1038,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 데이터셋 분리\n",
    "dataset_size = len(usd)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = int(0.1 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(usd, [train_size, val_size, test_size])\n",
    "\n",
    "# 데이터 로더 생성\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=True,\n",
    "                            num_workers=NUM_WORKERS,\n",
    "                            pin_memory=PIN_MEMORY)\n",
    "\n",
    "val_loader = DataLoader(dataset=val_dataset,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=True,\n",
    "                            num_workers=NUM_WORKERS,\n",
    "                            pin_memory=PIN_MEMORY)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=True,\n",
    "                            num_workers=NUM_WORKERS,\n",
    "                            pin_memory=PIN_MEMORY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1039,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100032-3-0-0.wav</td>\n",
       "      <td>100032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.317551</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100263-2-0-117.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>58.500000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100263-2-0-121.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>60.500000</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100263-2-0-126.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100263-2-0-137.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>68.500000</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11727</th>\n",
       "      <td>nothing_995.wav</td>\n",
       "      <td>333333</td>\n",
       "      <td>596.725488</td>\n",
       "      <td>598.725488</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11728</th>\n",
       "      <td>nothing_996.wav</td>\n",
       "      <td>333333</td>\n",
       "      <td>1736.118639</td>\n",
       "      <td>1738.118639</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11729</th>\n",
       "      <td>nothing_997.wav</td>\n",
       "      <td>333333</td>\n",
       "      <td>1472.959002</td>\n",
       "      <td>1474.959002</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11730</th>\n",
       "      <td>nothing_998.wav</td>\n",
       "      <td>333333</td>\n",
       "      <td>248.747211</td>\n",
       "      <td>250.747211</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11731</th>\n",
       "      <td>nothing_999.wav</td>\n",
       "      <td>333333</td>\n",
       "      <td>1475.270703</td>\n",
       "      <td>1477.270703</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>nothing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11732 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          slice_file_name    fsID        start          end  salience  fold  \\\n",
       "0        100032-3-0-0.wav  100032     0.000000     0.317551         1     5   \n",
       "1      100263-2-0-117.wav  100263    58.500000    62.500000         1     5   \n",
       "2      100263-2-0-121.wav  100263    60.500000    64.500000         1     5   \n",
       "3      100263-2-0-126.wav  100263    63.000000    67.000000         1     5   \n",
       "4      100263-2-0-137.wav  100263    68.500000    72.500000         1     5   \n",
       "...                   ...     ...          ...          ...       ...   ...   \n",
       "11727     nothing_995.wav  333333   596.725488   598.725488         1    11   \n",
       "11728     nothing_996.wav  333333  1736.118639  1738.118639         1    11   \n",
       "11729     nothing_997.wav  333333  1472.959002  1474.959002         1    11   \n",
       "11730     nothing_998.wav  333333   248.747211   250.747211         1    11   \n",
       "11731     nothing_999.wav  333333  1475.270703  1477.270703         1    11   \n",
       "\n",
       "       classID             class  \n",
       "0            3          dog_bark  \n",
       "1            2  children_playing  \n",
       "2            2  children_playing  \n",
       "3            2  children_playing  \n",
       "4            2  children_playing  \n",
       "...        ...               ...  \n",
       "11727       13           nothing  \n",
       "11728       13           nothing  \n",
       "11729       13           nothing  \n",
       "11730       13           nothing  \n",
       "11731       13           nothing  \n",
       "\n",
       "[11732 rows x 8 columns]"
      ]
     },
     "execution_count": 1039,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANNOTATIONS_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1040,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.UrbanSoundDataset at 0x329a87fd0>"
      ]
     },
     "execution_count": 1040,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1041,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 1041,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owo/anaconda3/envs/torchenv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/owo/anaconda3/envs/torchenv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# ResNet18 모델 설정\n",
    "model = resnet18(pretrained=False)\n",
    "# Change the output neurons of the model\n",
    "model.fc = nn.Linear(512, len(selected_labels)) \n",
    "# Use multiple GPUs if available\n",
    "if torch.cuda.device_count() > 1:\n",
    "  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "  model = nn.DataParallel(model)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1043,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수와 옵티마이저 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1044,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습률 스케줄러 설정\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Lists to store the training and validation loss and accuracy for each epoch\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "val_loss_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "# 최고 검증 정확도를 저장하기 위한 변수 설정\n",
    "best_acc = 0.0\n",
    "\n",
    "# 모델 훈련 함수 정의\n",
    "def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs):\n",
    "    global best_acc\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # 각 에포크(epoch)은 학습 단계와 검증 단계를 거칩니다.\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # 모델을 학습 모드로 설정\n",
    "            else:\n",
    "                model.eval()   # 모델을 평가 모드로 설정\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # 데이터를 반복\n",
    "            for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                #print(inputs.shape)\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # 매개변수 경사도를 0으로 설정\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 순전파\n",
    "                # 학습 시에만 연산 기록을 추적\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # 학습 단계인 경우 역전파 + 최적화\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # 통계를 계산\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.float() / dataset_sizes[phase]\n",
    "\n",
    "            # Save loss and accuracy for plotting\n",
    "            if phase == 'train':\n",
    "                train_loss_list.append(epoch_loss)\n",
    "                train_acc_list.append(epoch_acc)\n",
    "            else:\n",
    "                val_loss_list.append(epoch_loss)\n",
    "                val_acc_list.append(epoch_acc)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            \n",
    "            # 모델을 깊은 복사(deep copy)함\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                # Save model at best acc\n",
    "                torch.save(model.state_dict(),\"ResNet18_02.pth\")\n",
    "                print('model saved')\n",
    "\n",
    "\n",
    "\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # 가장 나은 모델 가중치를 불러옴\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1046,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 318/318 [00:47<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6842 Acc: 0.7678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:03<00:00, 11.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.4414 Acc: 0.8409\n",
      "model saved\n",
      "Epoch 1/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 318/318 [00:48<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3482 Acc: 0.8848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:03<00:00, 11.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.3201 Acc: 0.8913\n",
      "model saved\n",
      "Epoch 2/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 318/318 [00:48<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2442 Acc: 0.9135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:03<00:00, 11.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2769 Acc: 0.9055\n",
      "model saved\n",
      "Epoch 3/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 318/318 [00:48<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1936 Acc: 0.9343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:03<00:00, 11.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2136 Acc: 0.9417\n",
      "model saved\n",
      "Epoch 4/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 318/318 [00:47<00:00,  6.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1553 Acc: 0.9471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:03<00:00, 11.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2826 Acc: 0.9024\n",
      "Epoch 5/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 318/318 [00:47<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1293 Acc: 0.9562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:03<00:00, 11.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1290 Acc: 0.9559\n",
      "model saved\n",
      "Epoch 6/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 318/318 [00:47<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1093 Acc: 0.9640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:03<00:00, 11.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1273 Acc: 0.9559\n",
      "Epoch 7/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 318/318 [00:47<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0462 Acc: 0.9837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:03<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0632 Acc: 0.9795\n",
      "model saved\n",
      "Epoch 8/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 318/318 [00:47<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0295 Acc: 0.9912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:03<00:00, 11.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0653 Acc: 0.9748\n",
      "Epoch 9/14\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 76/318 [00:11<00:36,  6.67it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1046], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m# define dataset_sizes\u001b[39;00m\n\u001b[1;32m      6\u001b[0m dataset_sizes \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mlen\u001b[39m(train_dataset), \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mlen\u001b[39m(val_dataset)}\n\u001b[0;32m----> 8\u001b[0m best_model \u001b[39m=\u001b[39m train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs\u001b[39m=\u001b[39mnb_epochs)\n",
      "Cell \u001b[0;32mIn[1045], line 43\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloaders, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39m# 순전파\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m# 학습 시에만 연산 기록을 추적\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(phase \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m---> 43\u001b[0m     outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     44\u001b[0m     _, preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs, \u001b[39m1\u001b[39m)\n\u001b[1;32m     45\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_impl(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/torchvision/models/resnet.py:269\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_impl\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    267\u001b[0m     \u001b[39m# See note [TorchScript super()]\u001b[39;00m\n\u001b[1;32m    268\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)\n\u001b[0;32m--> 269\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(x)\n\u001b[1;32m    270\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m    271\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrack_running_stats \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[39minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39mbackends\u001b[39m.\u001b[39mcudnn\u001b[39m.\u001b[39menabled\n\u001b[1;32m   2452\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import copy\n",
    "nb_epochs = 15\n",
    "# 모델 훈련 시작\n",
    "dataloaders = {\"train\": train_loader, \"val\": val_loader}\n",
    "# define dataset_sizes\n",
    "dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n",
    "\n",
    "best_model = train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=nb_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.727431660731196,\n",
       " 0.36968730759817553,\n",
       " 0.2550020909747691,\n",
       " 0.21711624970544618,\n",
       " 0.15223796230001427,\n",
       " 0.14776002003573616,\n",
       " 0.11260692315154888,\n",
       " 0.051850783336150265,\n",
       " 0.02820283025349342,\n",
       " 0.028780429478261562,\n",
       " 0.022046837345780303,\n",
       " 0.018286623173914086,\n",
       " 0.014879207088027379,\n",
       " 0.01325036875813056,\n",
       " 0.010080309315122967]"
      ]
     },
     "execution_count": 984,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3232512068560743,\n",
       " 0.2895553507204131,\n",
       " 0.2364481827053498,\n",
       " 0.16886081314227713,\n",
       " 0.15768002457623406,\n",
       " 0.26134722770199065,\n",
       " 0.15649939959384795,\n",
       " 0.08112567498004448,\n",
       " 0.0826889955429507,\n",
       " 0.08403260992018609,\n",
       " 0.08701170154399876,\n",
       " 0.08290973585300646,\n",
       " 0.08786622649462028,\n",
       " 0.0790336536755052,\n",
       " 0.08015996387895695]"
      ]
     },
     "execution_count": 985,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_list = torch.tensor(train_acc_list).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc_list = torch.tensor(val_acc_list).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1047,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (15,) and (9,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1047], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m12\u001b[39m, \u001b[39m6\u001b[39m))\n\u001b[1;32m      5\u001b[0m plt\u001b[39m.\u001b[39msubplot(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m plt\u001b[39m.\u001b[39mplot(\u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, nb_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), train_loss_list, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTrain\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m plt\u001b[39m.\u001b[39mplot(\u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, nb_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), val_loss_list, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mValidation\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m'\u001b[39m\u001b[39mEpoch\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/matplotlib/pyplot.py:2812\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2810\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mplot)\n\u001b[1;32m   2811\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot\u001b[39m(\u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, scaley\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 2812\u001b[0m     \u001b[39mreturn\u001b[39;00m gca()\u001b[39m.\u001b[39mplot(\n\u001b[1;32m   2813\u001b[0m         \u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39mscalex, scaley\u001b[39m=\u001b[39mscaley,\n\u001b[1;32m   2814\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m({\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m: data} \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/matplotlib/axes/_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1685\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[0;32m-> 1688\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[1;32m   1689\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[1;32m   1690\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/matplotlib/axes/_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[1;32m    310\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 311\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_plot_args(\n\u001b[1;32m    312\u001b[0m     this, kwargs, ambiguous_fmt_datakey\u001b[39m=\u001b[39mambiguous_fmt_datakey)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/matplotlib/axes/_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39myaxis\u001b[39m.\u001b[39mupdate_units(y)\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[0;32m--> 504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y must have same first dimension, but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhave shapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    506\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    507\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y can be no greater than 2D, but have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (15,) and (9,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training and validation loss and accuracy\n",
    "# Move tensors to CPU\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, nb_epochs + 1), train_loss_list, label='Train')\n",
    "plt.plot(range(1, nb_epochs + 1), val_loss_list, label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, nb_epochs + 1), train_acc_list, label='Train')\n",
    "plt.plot(range(1, nb_epochs + 1), val_acc_list, label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1048,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가 함수 정의\n",
    "def test_model(model, test_loader, device):\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # 그래디언트 계산 비활성화\n",
    "        for inputs, labels in tqdm(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the model on test images: {100 * correct / total}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1050,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:03<00:00, 11.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on test images: 98.11616954474097%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 훈련 및 평가\n",
    "test_model(best_model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of the model on test images: 98.11616954474097%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
