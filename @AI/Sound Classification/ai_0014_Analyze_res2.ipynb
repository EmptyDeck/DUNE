{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owo/anaconda3/envs/torchenv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/owo/anaconda3/envs/torchenv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully loaded.+CPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import sounddevice as sd\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "from torch.autograd import Variable\n",
    "from IPython.display import Audio\n",
    "\n",
    "selected_labels = [\"door_nock\",\"glass_shatter\",\"car_horn\",\"dog_bark\",\"drilling\",\"nothing\",\"siren\",\"nothing2\"]\n",
    "\n",
    "\n",
    "try:\n",
    "    # MULTI GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = resnet18(pretrained=False)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(512,len(selected_labels))\n",
    "    model = nn.DataParallel(model)  # Add this line\n",
    "    model.load_state_dict(torch.load('ResNet18_02.pth', map_location=device))\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "\n",
    "    state_dict = torch.load('ResNet18_02.pth', map_location=device)\n",
    "    new_state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "    model.load_state_dict(new_state_dict)\n",
    "\n",
    "    print(\"Model successfully loaded. + GPU\")\n",
    "except:\n",
    "    #One GPU or CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = resnet18(pretrained=False)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(512,len(selected_labels))\n",
    "    try:\n",
    "        state_dict = torch.load('ResNet18_02.pth', map_location=device)\n",
    "        new_state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "        model.load_state_dict(new_state_dict)\n",
    "        model = model.to(device)\n",
    "        model = model.eval()\n",
    "        print(\"Model successfully loaded.+CPU\")\n",
    "    except:\n",
    "        print(\"Failed to load the model. Please check the model file.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 22050\n",
    "\n",
    "class MonoToColor(nn.Module):\n",
    "    def __init__(self, num_channels=3):\n",
    "        super(MonoToColor, self).__init__()\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        return tensor.repeat(self.num_channels, 1, 1)\n",
    "\n",
    "# Apply the same transformation as used during training\n",
    "transformation = transforms.Compose([\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_mels=128),\n",
    "    torchaudio.transforms.AmplitudeToDB(stype='power', top_db=80),\n",
    "    MonoToColor()\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2seconds / 80% upper guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def continuous_sound_prediction(model, device, transformation, sample_rate, target_sample_rate):\n",
    "    # Define class labels\n",
    "    count = 0\n",
    "    while True:\n",
    "        count = count + 1\n",
    "        # Record a 2 seconds mono audio at the specified sample rate\n",
    "        duration = 2.0  # seconds\n",
    "        recording = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1) \n",
    "        sd.wait()\n",
    "\n",
    "        # Convert to PyTorch tensor and switch channels and frames\n",
    "        recording = torch.from_numpy(recording).float()\n",
    "        recording = torch.transpose(recording, 0, 1)\n",
    "\n",
    "        # Resample if necessary\n",
    "        if sample_rate != target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sample_rate, target_sample_rate)\n",
    "            recording = resampler(recording)\n",
    "\n",
    "        # Mix down if necessary\n",
    "        if recording.shape[0] > 1:\n",
    "            recording = torch.mean(recording, dim=0, keepdim=True)\n",
    "\n",
    "        # Cut or pad if necessary\n",
    "        if recording.shape[1] > target_sample_rate:\n",
    "            recording = recording[:, :target_sample_rate]\n",
    "        elif recording.shape[1] < target_sample_rate:\n",
    "            num_missing_samples = target_sample_rate - recording.shape[1]\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            recording = nn.functional.pad(recording, last_dim_padding)\n",
    "\n",
    "        # Apply transformation\n",
    "        recording = transformation(recording)\n",
    "\n",
    "        # Make the prediction\n",
    "        model.eval()  # set model to evaluation mode\n",
    "        with torch.no_grad():  # deactivate autograd engine to reduce memory usage and speed up computations\n",
    "            recording = recording.to(device)\n",
    "            outputs = model(recording[None, ...])\n",
    "            probabilities = F.softmax(outputs, dim=1)  # apply softmax to output\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Get predicted label and its corresponding probability\n",
    "        predicted_label = selected_labels[predicted.item()]\n",
    "        predicted_confidence = probabilities[0, predicted.item()].item()  # get the probability of the predicted class\n",
    "\n",
    "        # Only print the output if the confidence is greater than 80%\n",
    "        if predicted_confidence >= 0.0:\n",
    "            print(f\"\\r {predicted_label} {predicted_confidence:.2%} count : {count}\", end=\"\")\n",
    "\n",
    "        # Sleep for 2 seconds before the next prediction\n",
    "        #time.sleep(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print every labels\n",
    "def continuous_sound_prediction(model, device, transformation, sample_rate, target_sample_rate):\n",
    "    # Define class labels\n",
    "\n",
    "    count = 0\n",
    "    while True:\n",
    "        count = count + 1\n",
    "        # Record a 2 seconds mono audio at the specified sample rate\n",
    "        duration = 2.0  # seconds\n",
    "        recording = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1) \n",
    "        sd.wait()\n",
    "\n",
    "        # Convert to PyTorch tensor and switch channels and frames\n",
    "        recording = torch.from_numpy(recording).float()\n",
    "        recording = torch.transpose(recording, 0, 1)\n",
    "\n",
    "        # Resample if necessary\n",
    "        if sample_rate != target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sample_rate, target_sample_rate)\n",
    "            recording = resampler(recording)\n",
    "\n",
    "        # Mix down if necessary\n",
    "        if recording.shape[0] > 1:\n",
    "            recording = torch.mean(recording, dim=0, keepdim=True)\n",
    "\n",
    "        # Cut or pad if necessary\n",
    "        if recording.shape[1] > target_sample_rate:\n",
    "            recording = recording[:, :target_sample_rate]\n",
    "        elif recording.shape[1] < target_sample_rate:\n",
    "            num_missing_samples = target_sample_rate - recording.shape[1]\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            recording = nn.functional.pad(recording, last_dim_padding)\n",
    "\n",
    "        # Apply transformation\n",
    "        recording = transformation(recording)\n",
    "\n",
    "        # Make the prediction\n",
    "        model.eval()  # set model to evaluation mode\n",
    "        with torch.no_grad():  # deactivate autograd engine to reduce memory usage and speed up computations\n",
    "            recording = recording.to(device)\n",
    "            outputs = model(recording[None, ...])\n",
    "            probabilities = F.softmax(outputs, dim=1)  # apply softmax to output\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Get predicted label and its corresponding probability\n",
    "        predicted_label = selected_labels[predicted.item()]\n",
    "        predicted_confidence = probabilities[0, predicted.item()].item()  # get the probability of the predicted class\n",
    "\n",
    "        ######## Adjust 'x' probability   #########\n",
    "        change_label = \"drilling\"\n",
    "        change_probability = 0.5\n",
    "        try:\n",
    "            x_index = selected_labels.index(change_label)\n",
    "            probabilities[0, x_index] = max(0.0, probabilities[0, x_index].item() - change_probability)\n",
    "        except:\n",
    "            pass\n",
    "        # Print the probabilities of all labels in one line\n",
    "        prob_strs = [f\"{label} {probabilities[0, idx].item():.2%}\" for idx, label in enumerate(selected_labels)]\n",
    "        print(f\"/ \".join(prob_strs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device :  cpu\n",
      "door_nock 0.02%/ glass_shatter 1.24%/ car_horn 0.03%/ dog_bark 0.05%/ drilling 48.62%/ nothing 0.00%/ siren 0.03%/ nothing2 0.00%\n",
      "door_nock 0.02%/ glass_shatter 1.24%/ car_horn 0.03%/ dog_bark 0.05%/ drilling 48.62%/ nothing 0.00%/ siren 0.03%/ nothing2 0.00%\n",
      "door_nock 0.02%/ glass_shatter 1.24%/ car_horn 0.03%/ dog_bark 0.05%/ drilling 48.62%/ nothing 0.00%/ siren 0.03%/ nothing2 0.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Call the continuous sound prediction function\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#print(\"model : \",model)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdevice : \u001b[39m\u001b[39m\"\u001b[39m,device)\n\u001b[0;32m----> 5\u001b[0m continuous_sound_prediction(model, device, transformation, SAMPLE_RATE, SAMPLE_RATE)\n",
      "Cell \u001b[0;32mIn[22], line 11\u001b[0m, in \u001b[0;36mcontinuous_sound_prediction\u001b[0;34m(model, device, transformation, sample_rate, target_sample_rate)\u001b[0m\n\u001b[1;32m      9\u001b[0m duration \u001b[39m=\u001b[39m \u001b[39m2.0\u001b[39m  \u001b[39m# seconds\u001b[39;00m\n\u001b[1;32m     10\u001b[0m recording \u001b[39m=\u001b[39m sd\u001b[39m.\u001b[39mrec(\u001b[39mint\u001b[39m(duration \u001b[39m*\u001b[39m sample_rate), samplerate\u001b[39m=\u001b[39msample_rate, channels\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \n\u001b[0;32m---> 11\u001b[0m sd\u001b[39m.\u001b[39mwait()\n\u001b[1;32m     13\u001b[0m \u001b[39m# Convert to PyTorch tensor and switch channels and frames\u001b[39;00m\n\u001b[1;32m     14\u001b[0m recording \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(recording)\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/sounddevice.py:395\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(ignore_errors)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wait for `play()`/`rec()`/`playrec()` to be finished.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \n\u001b[1;32m    381\u001b[0m \u001b[39mPlayback/recording can be stopped with a `KeyboardInterrupt`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m \n\u001b[1;32m    393\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[39mif\u001b[39;00m _last_callback:\n\u001b[0;32m--> 395\u001b[0m     \u001b[39mreturn\u001b[39;00m _last_callback\u001b[39m.\u001b[39mwait(ignore_errors)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/sounddevice.py:2601\u001b[0m, in \u001b[0;36m_CallbackContext.wait\u001b[0;34m(self, ignore_errors)\u001b[0m\n\u001b[1;32m   2595\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wait for finished_callback.\u001b[39;00m\n\u001b[1;32m   2596\u001b[0m \n\u001b[1;32m   2597\u001b[0m \u001b[39mCan be interrupted with a KeyboardInterrupt.\u001b[39;00m\n\u001b[1;32m   2598\u001b[0m \n\u001b[1;32m   2599\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2600\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2601\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevent\u001b[39m.\u001b[39mwait()\n\u001b[1;32m   2602\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   2603\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream\u001b[39m.\u001b[39mclose(ignore_errors)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/threading.py:622\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    620\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    621\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 622\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cond\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    623\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Call the continuous sound prediction function\n",
    "#print(\"model : \",model)\n",
    "print(\"device : \",device)\n",
    "\n",
    "continuous_sound_prediction(model, device, transformation, SAMPLE_RATE, SAMPLE_RATE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
