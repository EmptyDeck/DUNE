{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UI4\n",
    "updates\n",
    "1. Better Sound Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mic device pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device #0 name: Microsoft Sound Mapper - Input\n",
      "Device #1 name: Microphone(High Definition Audi\n",
      "Device #6 name: 주 사운드 캡처 드라이버\n",
      "Device #7 name: Microphone(High Definition Audio Device)\n",
      "Device #15 name: Microphone(High Definition Audio Device)\n",
      "Device #18 name: 마이크 (HD Audio Microphone)\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "\n",
    "def list_input_devices():\n",
    "    devices = sd.query_devices()\n",
    "    for i, device in enumerate(devices):\n",
    "        if device['max_input_channels'] > 0:  # this is an input device\n",
    "            print(f\"Device #{i} name: {device['name']}\")\n",
    "\n",
    "# List available input devices (including microphones)\n",
    "list_input_devices()\n",
    "device_id = 2 # <<<  -  pick your mic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mic Test\n",
    "MIC TEST type 1 if you want to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "PortAudioError",
     "evalue": "Error opening InputStream: Invalid number of channels [PaErrorCode -9998]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPortAudioError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m     buffer_index \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m frames\n\u001b[0;32m     23\u001b[0m \u001b[39m# Create a stream object\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m stream \u001b[39m=\u001b[39m sd\u001b[39m.\u001b[39;49mInputStream(callback\u001b[39m=\u001b[39;49maudio_callback, device\u001b[39m=\u001b[39;49mdevice_id, channels\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, samplerate\u001b[39m=\u001b[39;49m\u001b[39m44100\u001b[39;49m)\n\u001b[0;32m     26\u001b[0m \u001b[39m# Start the stream\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39mwith\u001b[39;00m stream:\n\u001b[0;32m     28\u001b[0m     \u001b[39m# Record for 3 seconds\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\homeyyj\\lib\\site-packages\\sounddevice.py:1421\u001b[0m, in \u001b[0;36mInputStream.__init__\u001b[1;34m(self, samplerate, blocksize, device, channels, dtype, latency, extra_settings, callback, finished_callback, clip_off, dither_off, never_drop_input, prime_output_buffers_using_stream_callback)\u001b[0m\n\u001b[0;32m   1391\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, samplerate\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, blocksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1392\u001b[0m              device\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, channels\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, latency\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1393\u001b[0m              extra_settings\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, finished_callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1394\u001b[0m              clip_off\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dither_off\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, never_drop_input\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1395\u001b[0m              prime_output_buffers_using_stream_callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1396\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"PortAudio input stream (using NumPy).\u001b[39;00m\n\u001b[0;32m   1397\u001b[0m \n\u001b[0;32m   1398\u001b[0m \u001b[39m    This has the same methods and attributes as `Stream`, except\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1419\u001b[0m \n\u001b[0;32m   1420\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1421\u001b[0m     _StreamBase\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39mself\u001b[39;49m, kind\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m'\u001b[39;49m, wrap_callback\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39marray\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m   1422\u001b[0m                          \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_remove_self(\u001b[39mlocals\u001b[39;49m()))\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\homeyyj\\lib\\site-packages\\sounddevice.py:898\u001b[0m, in \u001b[0;36m_StreamBase.__init__\u001b[1;34m(self, kind, samplerate, blocksize, device, channels, dtype, latency, extra_settings, callback, finished_callback, clip_off, dither_off, never_drop_input, prime_output_buffers_using_stream_callback, userdata, wrap_callback)\u001b[0m\n\u001b[0;32m    896\u001b[0m     userdata \u001b[39m=\u001b[39m _ffi\u001b[39m.\u001b[39mNULL\n\u001b[0;32m    897\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ptr \u001b[39m=\u001b[39m _ffi\u001b[39m.\u001b[39mnew(\u001b[39m'\u001b[39m\u001b[39mPaStream**\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 898\u001b[0m _check(_lib\u001b[39m.\u001b[39;49mPa_OpenStream(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ptr, iparameters, oparameters,\n\u001b[0;32m    899\u001b[0m                           samplerate, blocksize, stream_flags,\n\u001b[0;32m    900\u001b[0m                           callback_ptr, userdata),\n\u001b[0;32m    901\u001b[0m        \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mError opening \u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    903\u001b[0m \u001b[39m# dereference PaStream** --> PaStream*\u001b[39;00m\n\u001b[0;32m    904\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ptr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ptr[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\homeyyj\\lib\\site-packages\\sounddevice.py:2747\u001b[0m, in \u001b[0;36m_check\u001b[1;34m(err, msg)\u001b[0m\n\u001b[0;32m   2744\u001b[0m     hosterror_info \u001b[39m=\u001b[39m host_api, info\u001b[39m.\u001b[39merrorCode, hosterror_text\n\u001b[0;32m   2745\u001b[0m     \u001b[39mraise\u001b[39;00m PortAudioError(errormsg, err, hosterror_info)\n\u001b[1;32m-> 2747\u001b[0m \u001b[39mraise\u001b[39;00m PortAudioError(errormsg, err)\n",
      "\u001b[1;31mPortAudioError\u001b[0m: Error opening InputStream: Invalid number of channels [PaErrorCode -9998]"
     ]
    }
   ],
   "source": [
    "#MIC TEST type 1 if you want to test\n",
    "test = 1\n",
    "if test == 1:\n",
    "    import numpy as np\n",
    "\n",
    "    # Choose the device to use for recording\n",
    "    duration = 2  # seconds\n",
    "\n",
    "    # Create a buffer to store the audio data\n",
    "    buffer = np.zeros((duration * 44100,))\n",
    "    buffer_index = 0\n",
    "\n",
    "    # Define a callback function to process the audio input\n",
    "    def audio_callback(indata, frames, time, status):\n",
    "        global buffer_index\n",
    "        volume_norm = np.linalg.norm(indata) * 10\n",
    "        print(f'\\r{\"|\" * int(volume_norm)}', end='')  # print a simple \"volume bar\"\n",
    "\n",
    "        # Store the incoming data in the buffer\n",
    "        buffer[buffer_index:buffer_index+frames] = indata[:, 0]\n",
    "        buffer_index += frames\n",
    "\n",
    "    # Create a stream object\n",
    "    stream = sd.InputStream(callback=audio_callback, device=device_id, channels=1, samplerate=44100)\n",
    "\n",
    "    # Start the stream\n",
    "    with stream:\n",
    "        # Record for 3 seconds\n",
    "        sd.sleep(duration * 1000)\n",
    "\n",
    "    # Play back the recorded sound\n",
    "    sd.play(buffer, samplerate=44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick the labels that you want to use here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sound_rate (only raspberrypi use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = '''pcm.!default {\n",
    "#     type asym\n",
    "#     capture.pcm \"mic\"\n",
    "#     playback.pcm \"speaker\"\n",
    "# }\n",
    "# pcm.mic {\n",
    "#     type plug\n",
    "#     slave {\n",
    "#         pcm \"hw:2,0\"\n",
    "#     }\n",
    "# }\n",
    "# pcm.speaker {\n",
    "#     type plug\n",
    "#     slave {\n",
    "#         pcm \"hw:1,0\"\n",
    "#     }\n",
    "# }'''\n",
    "\n",
    "# # 파일 경로와 이름 설정\n",
    "# file_path = \"/home/pi/.asoundrc\"\n",
    "\n",
    "# # 텍스트 파일 생성 및 저장\n",
    "# with open(file_path, \"w\") as file:\n",
    "#     file.write(text)\n",
    "\n",
    "# print(\"Text saved to:\", file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to the model: ../@AI/AI/models/Batch4-1k_004.pth\n"
     ]
    }
   ],
   "source": [
    "# Relative path to the model file\n",
    "path_to_model = '../@AI/AI/models/Batch4-1k_004.pth'\n",
    "print(\"Path to the model:\", path_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully loaded deivce :  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\homeyyj\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Anaconda\\envs\\homeyyj\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "#One GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = resnet18(pretrained=False)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(512, 14)\n",
    "try:\n",
    "    state_dict = torch.load(path_to_model, map_location=device)\n",
    "    new_state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "    print(\"Model successfully loaded deivce : \",device)\n",
    "except:\n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    print(\"Failed to load the model. Please check the model file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\homeyyj\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "from torchvision import transforms  # Import the transforms module\n",
    "\n",
    "SAMPLE_RATE = 22050\n",
    "\n",
    "# Normalize\n",
    "class MinMaxNormalize(nn.Module):\n",
    "    def __init__(self, min_val=None, max_val=None):\n",
    "        super(MinMaxNormalize, self).__init__()\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        if self.min_val is None or self.max_val is None:\n",
    "            min_val = torch.min(tensor)\n",
    "            max_val = torch.max(tensor)\n",
    "        else:\n",
    "            min_val = self.min_val\n",
    "            max_val = self.max_val\n",
    "        \n",
    "        normalized_tensor = (tensor - min_val) / (max_val - min_val)\n",
    "        return normalized_tensor\n",
    "\n",
    "\n",
    "# Mono To Color (for 3 channels)\n",
    "class MonoToColor(nn.Module):\n",
    "    def __init__(self, num_channels=3):\n",
    "        super(MonoToColor, self).__init__()\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        return tensor.repeat(self.num_channels, 1, 1)\n",
    "\n",
    "# Transform + appy all func\n",
    "transformation = transforms.Compose([\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_mels=40),# higher the better but more complex. For talking we use 128, for sound effect, about 40.\n",
    "    torchaudio.transforms.AmplitudeToDB(stype='power', top_db=80),\n",
    "    MinMaxNormalize(),\n",
    "    MonoToColor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device #0 name: Microsoft Sound Mapper - Input\n",
      "Device #1 name: Microphone(High Definition Audi\n",
      "Device #6 name: 주 사운드 캡처 드라이버\n",
      "Device #7 name: Microphone(High Definition Audio Device)\n",
      "Device #15 name: Microphone(High Definition Audio Device)\n",
      "Device #18 name: 마이크 (HD Audio Microphone)\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import speech_recognition as sr\n",
    "import time\n",
    "\n",
    "def list_input_devices():\n",
    "    audio = pyaudio.PyAudio()\n",
    "    for i in range(audio.get_device_count()):\n",
    "        device_info = audio.get_device_info_by_index(i)\n",
    "        if device_info['maxInputChannels'] > 0:  # This is an input device\n",
    "            print(f\"Device #{i} name: {device_info['name']}\")\n",
    "\n",
    "def record_audio():\n",
    "    audio = pyaudio.PyAudio()\n",
    "    stream = audio.open(format=pyaudio.paInt16, channels=1, rate=16000, frames_per_buffer=1024, input=True , input_device_index=device_id)\n",
    "    frames = []\n",
    "    for i in range(0, int(16000 / 1024 * 3)):\n",
    "        data = stream.read(1024)\n",
    "        frames.append(data)\n",
    "    stream.close()\n",
    "    audio.terminate()  # close the PyAudio object\n",
    "    return b''.join(frames)\n",
    "def transcribe_audio(audio):\n",
    "    recognizer = sr.Recognizer()\n",
    "    audio_data = sr.AudioData(audio, 16000, 2)\n",
    "    try:\n",
    "        text = recognizer.recognize_sphinx(audio_data)\n",
    "        \n",
    "        return text\n",
    "    except:\n",
    "        return None\n",
    "def main(times):\n",
    "    for i in range(0, times):\n",
    "        audio = record_audio()\n",
    "        text = transcribe_audio(audio)\n",
    "        print(text)\n",
    "TIMES = 0\n",
    "if __name__ == \"__main__\":\n",
    "    main(TIMES)\n",
    "\n",
    "list_input_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOUND EFFECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device :  cpu\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch.nn.functional as F\n",
    "import sounddevice as sd\n",
    "global count\n",
    "count = 0\n",
    "print(\"device : \",device)\n",
    "## print every labels\n",
    "def continuous_sound_prediction(model, device, transformation, sample_rate):\n",
    "\n",
    "    global count\n",
    "\n",
    "    count = count + 1\n",
    "\n",
    "    labels = [\n",
    "        \"nothing a\", \"Car Horn\", \"Bell Ring\", \"Dog Bark\", \"nothing b\", \n",
    "        \"nothing c\", \"Glass Shatter\", \"Nothing d\", \"Nothing e\", \"Door Nock\", \n",
    "        \"Nothing f\", \"Nothing g\", \"Siren\", \"Nothing h\"\n",
    "    ]\n",
    "\n",
    "    duration = 2.0  # seconds\n",
    "    recording = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1, device=device_id)\n",
    "    sd.wait()\n",
    "    \n",
    "    # Preprocessing\n",
    "    recording = torch.from_numpy(recording).float().transpose(0, 1)\n",
    "    if recording.shape[0] > 1:\n",
    "        recording = torch.mean(recording, dim=0, keepdim=True)\n",
    "    recording = nn.functional.pad(recording, (0, sample_rate - recording.shape[1]))\n",
    "    \n",
    "    # Transformation\n",
    "    recording = transformation(recording)\n",
    "    \n",
    "    # Prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        recording = recording.to(device)\n",
    "        outputs = model(recording.unsqueeze(0))\n",
    "        #probabilities = F.softmax(outputs, dim=1)\n",
    "        probabilities = torch.sigmoid(outputs)\n",
    "\n",
    "        _, predicted = outputs.max(1)\n",
    "    # Print results\n",
    "    probs = [f\"{label} {prob:.2%}\" for label, prob in zip(labels, probabilities[0])]\n",
    "    print(f\"{count} / {' / '.join(probs)}\")\n",
    "\n",
    "    max_prob, predicted_label_idx = probabilities[0].max(0)\n",
    "    max_prob_label = labels[predicted_label_idx]\n",
    "\n",
    "    if max_prob > 0.0:\n",
    "        return max_prob_label, max_prob, probabilities\n",
    "    else:\n",
    "        return \"NONE\" , 0 , \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 결과 값 : \n",
      "RPi.GPIO import failed. No vibration\n",
      "pass vibration\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno -9998] Invalid number of channels",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 57\u001b[0m, in \u001b[0;36mSoundAnalysis.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m times \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m     56\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, times):\n\u001b[1;32m---> 57\u001b[0m     audio \u001b[39m=\u001b[39m record_audio()\n\u001b[0;32m     58\u001b[0m     text \u001b[39m=\u001b[39m transcribe_audio(audio)\n\u001b[0;32m     59\u001b[0m     \u001b[39mprint\u001b[39m(text)\n",
      "Cell \u001b[1;32mIn[8], line 14\u001b[0m, in \u001b[0;36mrecord_audio\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrecord_audio\u001b[39m():\n\u001b[0;32m     13\u001b[0m     audio \u001b[39m=\u001b[39m pyaudio\u001b[39m.\u001b[39mPyAudio()\n\u001b[1;32m---> 14\u001b[0m     stream \u001b[39m=\u001b[39m audio\u001b[39m.\u001b[39;49mopen(\u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49mpyaudio\u001b[39m.\u001b[39;49mpaInt16, channels\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, rate\u001b[39m=\u001b[39;49m\u001b[39m16000\u001b[39;49m, frames_per_buffer\u001b[39m=\u001b[39;49m\u001b[39m1024\u001b[39;49m, \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m , input_device_index\u001b[39m=\u001b[39;49mdevice_id)\n\u001b[0;32m     15\u001b[0m     frames \u001b[39m=\u001b[39m []\n\u001b[0;32m     16\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mint\u001b[39m(\u001b[39m16000\u001b[39m \u001b[39m/\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m3\u001b[39m)):\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\homeyyj\\lib\\site-packages\\pyaudio\\__init__.py:639\u001b[0m, in \u001b[0;36mPyAudio.open\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    632\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Opens a new stream.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \n\u001b[0;32m    634\u001b[0m \u001b[39m    See constructor for :py:func:`PyAudio.Stream.__init__` for parameter\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[39m    :returns: A new :py:class:`PyAudio.Stream`\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 639\u001b[0m     stream \u001b[39m=\u001b[39m PyAudio\u001b[39m.\u001b[39;49mStream(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    640\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_streams\u001b[39m.\u001b[39madd(stream)\n\u001b[0;32m    641\u001b[0m     \u001b[39mreturn\u001b[39;00m stream\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\homeyyj\\lib\\site-packages\\pyaudio\\__init__.py:441\u001b[0m, in \u001b[0;36mPyAudio.Stream.__init__\u001b[1;34m(self, PA_manager, rate, channels, format, input, output, input_device_index, output_device_index, frames_per_buffer, start, input_host_api_specific_stream_info, output_host_api_specific_stream_info, stream_callback)\u001b[0m\n\u001b[0;32m    438\u001b[0m     arguments[\u001b[39m'\u001b[39m\u001b[39mstream_callback\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m stream_callback\n\u001b[0;32m    440\u001b[0m \u001b[39m# calling pa.open returns a stream object\u001b[39;00m\n\u001b[1;32m--> 441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stream \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39;49mopen(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49marguments)\n\u001b[0;32m    443\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_latency \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stream\u001b[39m.\u001b[39minputLatency\n\u001b[0;32m    444\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_latency \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stream\u001b[39m.\u001b[39moutputLatency\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno -9998] Invalid number of channels"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode now :   True\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\homeyyj\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from PyQt5 import QtCore, QtGui, QtWidgets\n",
    "from PyQt5.QtCore import QThread, pyqtSignal\n",
    "from PyQt5.QtGui import QMovie\n",
    "\n",
    "from time import sleep\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pyaudio\n",
    "import torchaudio\n",
    "import sounddevice as sd\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "###### INIT ######\n",
    "p = pyaudio.PyAudio()\n",
    "sample_rate = SAMPLE_RATE\n",
    "target_sample_rate = SAMPLE_RATE\n",
    "mode = 0 ## 1 starts with sound effect, 0 starts with stt\n",
    "print(\" 결과 값 : \")\n",
    "\n",
    "\n",
    "# Sound Analysis class running on a separate thread\n",
    "class SoundAnalysis(QThread):\n",
    "    # Define a pyqtSignal with str type, which will be used to send the analysis results to the main thread\n",
    "    result_signal = pyqtSignal(str, float)  # Add a float type for the probability\n",
    "\n",
    "    def __init__(self, model, device, transformation, sample_rate):\n",
    "        QThread.__init__(self)\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.transformation = transformation\n",
    "        self.sample_rate = sample_rate\n",
    "        \n",
    "\n",
    "    def run(self):## Main code\n",
    "        count = 0\n",
    "        text_str = \"\"\n",
    "        while True:\n",
    "\n",
    "            if mode == True:\n",
    "                text_str = \"\" #empty the text strings\n",
    "\n",
    "                predicted_label, predicted_confidence, probabilities = continuous_sound_prediction(model, device, transformation, SAMPLE_RATE) \n",
    "                if mode == True:\n",
    "                    try:               \n",
    "                        self.result_signal.emit(predicted_label, predicted_confidence)\n",
    "                        #vibration()\n",
    "                    except:\n",
    "                        self.result_signal.emit(predicted_label, predicted_confidence)\n",
    "                    count = count + 1\n",
    "            if mode == False:\n",
    "\n",
    "                times = 2\n",
    "                for _ in range(0, times):\n",
    "                    audio = record_audio()\n",
    "                    text = transcribe_audio(audio)\n",
    "                    print(text)\n",
    "                    text_str += text + \" \"\n",
    "                    if mode == True: ## This is checking again if the button is pushed. If I dont add this, it will reset_label and show the text_str again before going to mode True\n",
    "                        break\n",
    "                    self.result_signal.emit(text_str,0)\n",
    "        \n",
    "    def reset_label(self):# Restting the labels when the button is pressed\n",
    "        self.result_signal.emit(\"켜는중\", 0)\n",
    "        \n",
    "class Ui_MainWindow(object):\n",
    "    def setupUi(self, MainWindow):\n",
    "        MainWindow.setObjectName(\"MainWindow\")\n",
    "        MainWindow.resize(640, 480)\n",
    "        self.received_text = \"\"\n",
    "        self.centralwidget = QtWidgets.QWidget(MainWindow)\n",
    "        self.centralwidget.setObjectName(\"centralwidget\")\n",
    "        \n",
    "        # Create a QScrollArea\n",
    "        self.scrollArea = QtWidgets.QScrollArea(self.centralwidget)\n",
    "        self.scrollArea.setGeometry(QtCore.QRect(10, 300, 611, 121))\n",
    "        self.scrollArea.setWidgetResizable(True)\n",
    "        self.scrollArea.setObjectName(\"scrollArea\")\n",
    "\n",
    "        # Create the label inside the QScrollArea\n",
    "        self.label = QtWidgets.QLabel(self.scrollArea)\n",
    "        self.label.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        self.label.setObjectName(\"text_label\")\n",
    "        self.label.setWordWrap(True)\n",
    "        self.scrollArea.setWidget(self.label)\n",
    "\n",
    "        self.label_2 = QtWidgets.QLabel(self.centralwidget)\n",
    "        self.label_2.setGeometry(QtCore.QRect(10, 10, 441, 261))\n",
    "        self.label_2.setText(\"\")\n",
    "        self.label_2.setObjectName(\"imgae_image\")\n",
    "\n",
    "        self.pushButton = QtWidgets.QPushButton(self.centralwidget)\n",
    "        self.pushButton.setGeometry(QtCore.QRect(460, 10, 171, 111))\n",
    "        self.pushButton.setObjectName(\"pushButton\")\n",
    "        MainWindow.setCentralWidget(self.centralwidget)\n",
    "\n",
    "        #english_text_label\n",
    "        self.new_label = QtWidgets.QLabel(self.centralwidget)\n",
    "        self.new_label.setGeometry(QtCore.QRect(40, 200, 561, 201))\n",
    "        self.new_label.setText(\"\")\n",
    "        self.new_label.setAlignment(QtCore.Qt.AlignCenter)  # 텍스트 중앙 정렬\n",
    "        self.new_label.setObjectName(\"english_text_label\")\n",
    "        self.new_label.hide()  # Hide the new label initially\n",
    "        #english_image_label\n",
    "        self.new_label_2 = QtWidgets.QLabel(self.centralwidget)\n",
    "        self.new_label_2.setGeometry(QtCore.QRect(50, 40, 144, 130))\n",
    "        self.new_label_2.setPixmap(QtGui.QPixmap(\"english_image.png\"))\n",
    "        self.new_label_2.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        self.new_label_2.setObjectName(\"english_image_label\")\n",
    "        \n",
    "\n",
    "        MainWindow.setCentralWidget(self.centralwidget)\n",
    "        self.menubar = QtWidgets.QMenuBar(MainWindow)\n",
    "        self.menubar.setGeometry(QtCore.QRect(0, 0, 800, 29))\n",
    "        self.menubar.setObjectName(\"menubar\")\n",
    "        MainWindow.setMenuBar(self.menubar)\n",
    "        self.statusbar = QtWidgets.QStatusBar(MainWindow)\n",
    "        self.statusbar.setObjectName(\"statusbar\")\n",
    "        MainWindow.setStatusBar(self.statusbar)\n",
    "\n",
    "        self.retranslateUi(MainWindow)\n",
    "        QtCore.QMetaObject.connectSlotsByName(MainWindow)\n",
    "\n",
    "        self.pushButton.clicked.connect(self.onPushButtonClicked)\n",
    "\n",
    "        # Initialize SoundAnalysis and connect the result_signal with the updateLabel function\n",
    "        self.sound_analysis = SoundAnalysis(model, device, transformation, SAMPLE_RATE)\n",
    "        self.sound_analysis.result_signal.connect(self.updateLabel)\n",
    "        self.sound_analysis.result_signal.connect(self.updateLabel2)\n",
    "        self.sound_analysis.start()  # Start the sound analysis thread\n",
    "        \n",
    "        self.is_new_labels_visible = False\n",
    "\n",
    "        \n",
    "\n",
    "    def onPushButtonClicked(self): ######BUTTON#######\n",
    "        global mode\n",
    "        mode = not mode\n",
    "        self.new_label_2.hide()  # Hide the new label initially\n",
    "        self.sound_analysis.reset_label()#reset the screen (its connected to the sound analysis class)\n",
    "        print(\"mode now :  \",mode)\n",
    "\n",
    "    def updateLabel(self, predicted_label, predicted_confidence):\n",
    "        #print(\"Received signal\")  # Print message when signal is received\n",
    "        if predicted_confidence == 0:\n",
    "            self.label.setText(f\"{predicted_label}\")\n",
    "        else:\n",
    "            self.label.setText(f\"{predicted_label}  {predicted_confidence*100:.2f}%\")\n",
    "        #print(predicted_label)\n",
    "        \n",
    "        # After updating the text, ensure the QScrollArea scrolls to the bottom\n",
    "        self.scrollArea.verticalScrollBar().setValue(self.scrollArea.verticalScrollBar().maximum())\n",
    "\n",
    "        \n",
    "        \n",
    "    def updateLabel2(self, predicted_label):\n",
    "        relative_image_folder_path = \"../image_file/GIF\" #\\ only works for windows\n",
    "        image_folder_path= os.path.join(current_path, relative_image_folder_path)\n",
    "        full_file_name = os.path.join(image_folder_path, f\"{predicted_label}.gif\")\n",
    "        self.movie = QMovie(full_file_name)\n",
    "        self.label_2.setMovie(self.movie)\n",
    "        self.movie.start()\n",
    "\n",
    "        #print(predicted_label)\n",
    "\n",
    "\n",
    "    def retranslateUi(self, MainWindow):\n",
    "        _translate = QtCore.QCoreApplication.translate\n",
    "        MainWindow.setWindowTitle(_translate(\"MainWindow\", \"MainWindow\"))\n",
    "        self.label.setFont(QtGui.QFont(\"AppleSystemUIFont\",20))\n",
    "        self.label.setStyleSheet(\"Color : black\")\n",
    "        self.pushButton.setText(_translate(\"MainWindow\", \"PushButton\"))\n",
    "        \n",
    "try:\n",
    "    import RPi.GPIO as GPIO\n",
    "    print(\"RPi.GPIO loaded successfully\")\n",
    "    import threading\n",
    "    print(\"threading loaded successfully\")\n",
    "\n",
    "    class VibrationController:\n",
    "        def __init__(self):\n",
    "            # 핀 번호 설정\n",
    "            self.red_button_pin = 17\n",
    "            self.yellow_button_pin = 22\n",
    "            self.green_button_pin = 27\n",
    "            self.vibration_motor_pin = 18\n",
    "            \n",
    "            # 진동 세기 초기화\n",
    "            self.vibration_intensity_temp = 100\n",
    "            self.vibration_intensity = 0\n",
    "            \n",
    "            # 이전 스위치 상태 초기화\n",
    "            self.prev_red_button_state = GPIO.HIGH\n",
    "            self.prev_yellow_button_state = GPIO.HIGH\n",
    "            self.prev_green_button_state = GPIO.HIGH\n",
    "            \n",
    "            self.debounce_time = 0.2\n",
    "            \n",
    "            # 진동 상태를 저장하는 변수\n",
    "            self.vibration_on = False\n",
    "            \n",
    "            # GPIO 초기화\n",
    "            GPIO.setmode(GPIO.BCM)\n",
    "            GPIO.setup(self.red_button_pin, GPIO.IN, pull_up_down=GPIO.PUD_UP)\n",
    "            GPIO.setup(self.yellow_button_pin, GPIO.IN, pull_up_down=GPIO.PUD_UP)\n",
    "            GPIO.setup(self.green_button_pin, GPIO.IN, pull_up_down=GPIO.PUD_UP)\n",
    "            GPIO.setup(self.vibration_motor_pin, GPIO.OUT)\n",
    "            \n",
    "            # PWM 설정\n",
    "            self.pwm_frequency = 1000\n",
    "            self.pwm = GPIO.PWM(self.vibration_motor_pin, self.pwm_frequency)\n",
    "            self.pwm.start(self.vibration_intensity)\n",
    "        \n",
    "        def display_vibration_intensity(self):\n",
    "            print(f\"진동 세기: {self.vibration_intensity}\")\n",
    "        \n",
    "        def adjust_vibration_intensity(self, button_pin):\n",
    "            if button_pin == self.red_button_pin:\n",
    "                self.vibration_intensity = self.vibration_intensity_temp\n",
    "            elif button_pin == self.yellow_button_pin:\n",
    "                self.vibration_intensity = max(self.vibration_intensity - 10, 0)\n",
    "            elif button_pin == self.green_button_pin:\n",
    "                self.vibration_intensity = min(self.vibration_intensity + 10, 100)\n",
    "            \n",
    "            self.pwm.ChangeDutyCycle(self.vibration_intensity)\n",
    "            self.display_vibration_intensity()\n",
    "        \n",
    "        def run(self):\n",
    "            try:\n",
    "                while True:\n",
    "                    red_button_state = GPIO.input(self.red_button_pin)\n",
    "                    yellow_button_state = GPIO.input(self.yellow_button_pin)\n",
    "                    green_button_state = GPIO.input(self.green_button_pin)\n",
    "            \n",
    "                    if red_button_state != self.prev_red_button_state:\n",
    "                        time.sleep(self.debounce_time)\n",
    "                        if red_button_state != GPIO.input(self.red_button_pin):\n",
    "                            \n",
    "                            if not self.vibration_on:\n",
    "                                self.vibration_on = True\n",
    "                                self.adjust_vibration_intensity(self.red_button_pin)\n",
    "                            else:\n",
    "                                self.vibration_intensity_temp = self.vibration_intensity\n",
    "                                self.vibration_intensity = 0\n",
    "                                self.pwm.ChangeDutyCycle(self.vibration_intensity)\n",
    "                                self.display_vibration_intensity()\n",
    "                                self.vibration_on = False\n",
    "            \n",
    "                        self.prev_red_button_state = red_button_state\n",
    "            \n",
    "                    if yellow_button_state != self.prev_yellow_button_state:\n",
    "                        time.sleep(self.debounce_time)\n",
    "                        if yellow_button_state != GPIO.input(self.yellow_button_pin):\n",
    "                            self.adjust_vibration_intensity(self.yellow_button_pin)\n",
    "                        self.prev_yellow_button_state = yellow_button_state\n",
    "            \n",
    "                    if green_button_state != self.prev_green_button_state:\n",
    "                        time.sleep(self.debounce_time)\n",
    "                        if green_button_state != GPIO.input(self.green_button_pin):\n",
    "                            self.adjust_vibration_intensity(self.green_button_pin)\n",
    "                        self.prev_green_button_state = green_button_state\n",
    "            \n",
    "                    time.sleep(0.01)  \n",
    "            \n",
    "            except KeyboardInterrupt:\n",
    "                self.pwm.stop()\n",
    "                GPIO.cleanup()\n",
    "except:\n",
    "    print(\"RPi.GPIO import failed. No vibration\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    app = QtWidgets.QApplication(sys.argv)\n",
    "    MainWindow = QtWidgets.QMainWindow()\n",
    "    try:\n",
    "        vibration_controller = VibrationController()\n",
    "        vibration_thread = threading.Thread(target=vibration_controller.run)\n",
    "        vibration_thread.start()\n",
    "    except:\n",
    "        print(\"pass vibration\")\n",
    "    ui = Ui_MainWindow()\n",
    "    ui.setupUi(MainWindow)\n",
    "    MainWindow.show()\n",
    "    sys.exit(app.exec_())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 가능한 폰트 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyQt5.QtGui import QFontDatabase\n",
    "\n",
    "print(QFontDatabase().families())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
